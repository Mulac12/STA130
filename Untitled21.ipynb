{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8116ce3b",
   "metadata": {},
   "source": [
    "1. \n",
    "Type of Problem a Classification Decision Tree Addresses\n",
    "A Classification Decision Tree addresses classification problems, where the goal is to assign items into predefined categories or classes based on their features. The tree uses a series of decision rules derived from the training data to classify data points.\n",
    "Real-World Applications\n",
    "Healthcare: Predicting whether a patient has a disease (e.g., cancer) based on symptoms and test results (e.g., \"Has disease\" or \"Doesn't have disease\").\n",
    "Finance: Identifying if a transaction is fraudulent (e.g., \"Fraud\" or \"Not Fraud\").\n",
    "Marketing: Classifying customers as \"Likely to Buy\" or \"Not Likely to Buy\" based on past purchasing behavior.\n",
    "Education: Predicting whether a student will pass or fail based on attendance, grades, and other factors.\n",
    "Manufacturing: Detecting whether a product is defective or non-defective based on sensor data.\n",
    "    The primary difference between predictions made by a classification decision tree and multiple linear regression lies in the type of problem they address. Classification decision trees are used for classification tasks, predicting discrete class labels (e.g., \"Yes\" or \"No\"), while multiple linear regression is suited for regression problems, predicting continuous numerical values (e.g., 50.2, 105.3). Decision trees split the dataset into branches based on feature thresholds, assigning the majority class of training samples at each leaf node, thereby capturing non-linear relationships through recursive splitting. In contrast, multiple linear regression fits a linear equation \n",
    "𝑦=𝑏0+𝑏1x1+𝑏2𝑥2+⋯+𝑏𝑛𝑥𝑛, assuming a linear relationship between input features and the target variable. Interpretability also varies, with decision trees offering visually intuitive rules (e.g., \"If feature A > 5, then class is X\"), while regression requires understanding coefficients. Predictions from decision trees provide probabilities for classes, selecting the one with the highest probability, whereas regression directly outputs a single continuous value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebaeb18",
   "metadata": {},
   "source": [
    "2. Accuracy\n",
    "Application Scenario: General performance evaluation in balanced datasets, such as evaluating the effectiveness of a spam email filter when the dataset contains an approximately equal number of spam and non-spam emails.\n",
    "\n",
    "Rationale: Accuracy provides an overall measure of how well the model is performing. It is suitable when the cost of misclassifying positive and negative cases is similar and the dataset is balanced. In the spam filter example, accuracy indicates how often the filter correctly identifies emails as spam or not spam.\n",
    "\n",
    "Sensitivity (Recall)\n",
    "Application Scenario: Disease detection in medical diagnostics, such as identifying cases of cancer.\n",
    "\n",
    "Rationale: Sensitivity measures the model's ability to correctly identify actual positive cases (e.g., patients with cancer). This metric is crucial in scenarios where missing a positive case (false negative) can have severe consequences, such as failing to diagnose a treatable disease.\n",
    "\n",
    "Specificity\n",
    "Application Scenario: Fraud detection systems in finance, such as identifying fraudulent transactions in credit card payments.\n",
    "\n",
    "Rationale: Specificity measures the ability to correctly identify actual negative cases (e.g., legitimate transactions). It is critical in scenarios where incorrectly flagging negatives as positives (false positives) can lead to significant inconvenience or cost, such as blocking a legitimate transaction.\n",
    "\n",
    "Precision\n",
    "Application Scenario: Information retrieval in search engines, such as identifying relevant search results.\n",
    "\n",
    "Rationale: Precision measures how many of the predicted positives are actually correct. In a search engine context, high precision ensures that the results returned to a user are highly relevant, minimizing irrelevant links (false positives). This improves user satisfaction and system effectiveness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8003c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, recall_score, make_scorer\n",
    "import graphviz as gv\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/pointOfive/STA130_F23/main/Data/amazonbooks.csv\"\n",
    "ab = pd.read_csv(url, encoding=\"ISO-8859-1\")\n",
    "# create `ab_reduced_noNaN` based on the specs above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b733e7d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (517575123.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    3. Column Statistics:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "3. Column Statistics:\n",
    "\n",
    "Title: There are 309 unique book titles. The most frequent title is \"The Great Gatsby\" (appears 3 times).\n",
    "Author: There are 251 unique authors, with Jodi Picoult being the most frequent author (appears 7 times).\n",
    "List Price and Amazon Price:\n",
    "The average list price is $18.36, and the average Amazon price is $12.94.\n",
    "Prices range from $1.50 to $139.95.\n",
    "Hard_or_Paper: This categorical feature indicates whether the book is hardcover (H) or paperback (P). Paperbacks are more common (233 entries).\n",
    "NumPages: The average number of pages is 334, ranging from 24 to 896.\n",
    "Publisher: There are 158 unique publishers, with \"Vintage\" being the most frequent (37 entries).\n",
    "Pub year: Publication years range from 1936 to 2011, with an average publication year of 2002.\n",
    "ISBN-10: Contains unique identifiers for most books, with 316 unique values.\n",
    "Thick: Represents the book thickness, ranging from 0.1 to 2.1 (units unknown).\n",
    "Data Types:\n",
    "\n",
    "Numerical columns: List Price, Amazon Price, NumPages, Pub year, and Thick.\n",
    "Categorical column: Hard_or_Paper.\n",
    "Text columns: Title, Author, Publisher, and ISBN-10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b562d9b",
   "metadata": {},
   "source": [
    "4. The training dataset contains 255 observations, and the testing dataset contains 64 observations.\n",
    "\n",
    "Explanation of the Decision Tree\n",
    "The visualization above shows how the DecisionTreeClassifier splits the data based on the List Price feature to predict whether a book is hardcover or paperback (Hard_or_Paper). At each decision node:\n",
    "\n",
    "The tree evaluates a threshold on the List Price.\n",
    "Books with a price below or above the threshold are classified into one of the two categories (hardcover or paperback).\n",
    "The maximum depth of the tree is set to 2, which limits the tree to at most two splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e97674",
   "metadata": {},
   "source": [
    "5. Explanation of Predictions for the clf2 Model:\n",
    "The visualization shows the decision-making process for classifying books as either hardcover or paperback using three features: NumPages, Thick, and List Price. The tree follows these general steps:\n",
    "\n",
    "Splitting Criteria: At each decision node, the tree evaluates a threshold for one of the features (NumPages, Thick, or List Price) to partition the data.\n",
    "For example, \"Is NumPages > 300?\" or \"Is List Price ≤ $15?\"\n",
    "Recursive Splitting: The tree continues splitting the data at each level based on the feature thresholds, creating branches until a maximum depth of 4 is reached or a stopping condition (like no further improvement) is met.\n",
    "Leaf Nodes: The terminal nodes represent the final predictions. Each leaf node is labeled with the predicted class (Hardcover or Paperback) and the proportion of samples in that class.\n",
    "General Prediction Process:\n",
    "To predict for a new book:\n",
    "\n",
    "Start at the root node and evaluate the first feature condition (e.g., NumPages).\n",
    "Move to the next branch depending on whether the condition is true or false.\n",
    "Repeat this process until reaching a leaf node, where the final prediction is determined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68596a35",
   "metadata": {},
   "source": [
    "6. Performance Metrics for Each Model:\n",
    "Model clf (Using Only List Price)\n",
    "Accuracy: 84.38%\n",
    "Sensitivity (Recall for H): 70%\n",
    "Specificity (Recall for P): 90.91%\n",
    "Model clf2 (Using NumPages, Thick, and List Price)\n",
    "Accuracy: 93.75%\n",
    "Sensitivity (Recall for H): 90%\n",
    "Specificity (Recall for P): 95.45%\n",
    "Observations:\n",
    "Model clf2 performs better overall:\n",
    "\n",
    "It has higher accuracy, sensitivity, and specificity compared to clf.\n",
    "The inclusion of multiple features (NumPages, Thick, and List Price) likely provides more predictive power.\n",
    "Trade-offs in Model clf:\n",
    "\n",
    "Lower sensitivity indicates it misses more hardcover books (false negatives).\n",
    "Higher specificity means it performs relatively well in identifying paperback books (true negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77978b4",
   "metadata": {},
   "source": [
    "The differences between the two confusion matrices stem from the features used to train the models. The first confusion matrix uses only the List Price feature for predictions, which provides a limited perspective on the relationships between the data and the target variable. This simplicity may lead to poorer classification performance, especially if other features like NumPages and Thick carry significant predictive power.\n",
    "\n",
    "The second confusion matrix incorporates additional features (NumPages and Thick) along with List Price, enabling the model to make more informed splits and capture complex patterns in the data. This results in improved classification performance, reflected in higher sensitivity, specificity, and accuracy in the clf2 model compared to clf. The two confusion matrices above (for clf and clf2) on the test set are better because they evaluate the models on unseen data, providing a more accurate representation of their generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71074d1",
   "metadata": {},
   "source": [
    "8. The most important feature for making predictions in the clf2 model is List Price, with an importance score of approximately 0.494. This indicates that nearly half of the explanatory power of the model comes from List Price, making it the most influential predictor in the decision tree.\n",
    "\n",
    "The visualization above provides a clear view of how the other features (NumPages and Thick) contribute to the model, but their impact is comparatively less significant than List Price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b4d51c",
   "metadata": {},
   "source": [
    "Interpreting coefficients in linear regression is straightforward because each coefficient directly quantifies the change in the target variable for a one-unit increase in the corresponding predictor, assuming all other variables remain constant. This direct relationship allows for clear and quantitative understanding of the impact of each feature on the outcome. In contrast, feature importances in decision trees represent the relative contribution of each feature to the model’s predictive performance, based on the improvement in the splitting criterion (e.g., Gini impurity or entropy). This measure is less direct and does not provide an easily interpretable numerical relationship between the feature and the target variable, as it aggregates the feature’s influence across potentially complex and nonlinear interactions within the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30577952",
   "metadata": {},
   "source": [
    "chatbot summary: \n",
    "Linear regression coefficients provide a straightforward interpretation of the relationship between predictor variables and the target variable. Each coefficient quantifies the change in the predicted outcome for a one-unit increase in the corresponding predictor, assuming all other variables remain constant. This makes it easy to directly understand and communicate the influence of each feature on the target, as the model is inherently linear and additive.\n",
    "\n",
    "In contrast, feature importances in decision trees measure the relative contribution of each predictor variable to the overall performance of the model, based on how much they improve the splitting criterion (e.g., Gini impurity or entropy) across the tree’s structure. Feature importances capture complex, nonlinear relationships and interactions between variables, making them a powerful tool for understanding which features are most influential. However, their interpretation is less direct, as they aggregate the contributions of a feature over potentially many splits in the tree and do not represent a fixed numerical relationship between the feature and the target variable. Thus, while linear regression is more interpretable and transparent, decision trees are better suited for capturing intricate patterns but require caution to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4712a4",
   "metadata": {},
   "source": [
    "Chatgpt link: https://chatgpt.com/share/673fd090-7de0-8006-bdd0-e442b2d76821"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca47b04",
   "metadata": {},
   "source": [
    "9: no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d533d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
